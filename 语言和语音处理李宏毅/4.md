<img width="686" height="518" alt="image" src="https://github.com/user-attachments/assets/f6542be4-bcd6-40ae-96e9-6c4e45f745b9" />


CTC是Connectionist Temporal Classification的缩写，它可以做到线上实时地语音识别。它用的是单向的RNN，且只作为编码器，用MLP作为解码预测文字分布。RNN对当前语音输入xi编码成hi。MLP再对它乘上一个权重，接上Softmax，得到词表V+1大小的概率分布。但有时候当前的语音输入可能并不能对应实际的文本token，所以预测要额外多一个为空的类别，表示模型不知道要输出什么。

<img width="661" height="277" alt="image" src="https://github.com/user-attachments/assets/d0197e5a-aac8-43b0-bf0a-280a11beda01" />


TCT它没有下采样，所以输入和输出的序列长度都是T。模型预测完后，它还要进行后处理，一是把重复的token合并，二是空类别去掉，才是最终的预测序列。TCT的这种预测方式，会让它的数据标注变得很难，因为要确保刚好每个输入声音特征都对应一个正确的token。而标注语料的不足，会直接影响模型评测的表现。此外，一个序列正确的标注方式又可以存着很多种，造成标注多标准问题。这也加大标注数据选择的困难。

<img width="663" height="224" alt="image" src="https://github.com/user-attachments/assets/7a07b421-41cf-4594-8d6f-c66e54cc89f4" />


暴力一点，我们就穷举所有的可能标注去训练。如果在数据集充足情况下，CTC的效果还是不错的。

<img width="673" height="472" alt="image" src="https://github.com/user-attachments/assets/c0343d4c-a6d9-4298-a767-a0c9c73a33d0" />


CTC模型的问题主要在，每个MLP的解码器工作是独立的。它可能会遇到一个奇怪的问题。比如语音数据部分前三帧都在发音c，第一个隐层hi解码出是c后，后面两个隐层就无须再解码出c了。但由于它们是独立工作的，后面的解码部分不知道前面解码的是什么，所以第二个可能解码成空或c，第三个也可能解码成空或c。但如果编码器是足够深的RNN，它也可能在编码过程中考虑这种前后关系的依赖，让第一个输出隐层包含更多关于c的信息，而后面两个隐层包含更多关于空类别的信息。

<img width="660" height="312" alt="image" src="https://github.com/user-attachments/assets/67b5ba86-539b-496c-8e63-cdcecaf68ab3" />


我们来看下一个模型RNA。谷歌的学者发论文很喜欢玩取名的梗，这个模型与核糖核酸RNA同名。RNA是Recurrent Neural Aligner的缩写，它是一个介于CTC和RNN-T的过渡模型，针对CTC独立解码问题，把MLP换成了RNN。如果我们让解码的RNN是吃一个输入，输出多个token，比如th只是一个音。对于CTC而言，它只能先输出t再在下一个时间点解码输出h，而我们希望它直接输出t和h。这种把独立解码的MLP替换成RNN，且RNN可以输入一个隐层，预测多个tokens的模型便是RNN-T。RNN-T与CTC同样会遇到训练数据难标注，标注多标准的问题。我们会穷举所有可能的标注给模型训练。

<img width="665" height="519" alt="image" src="https://github.com/user-attachments/assets/e35632aa-5ac7-4641-b953-afc7030bd760" />


RNN-T在解码过程中是如何确保输入一个隐层，输出多个token的呢？输入一个ht，RNN-T会决定要产生一个"t"。这个"t"会被放到另一个RNN中，输出的隐层会放回RNN-T的解码器中作为隐层和之前的ht输入，来产生新的输出"h"。再反复一次。由于那另一个RNN看到过"h"了，就会输出新的东西。这个新东西再与之前的ht输入RNN-T，可能会得到一个空类别。而另一个RNN会无视掉这个空类别，把之前输出"h"，与下一个隐层ht+1输入到RNN-T得到的新输出"e"。

<img width="690" height="482" alt="image" src="https://github.com/user-attachments/assets/819902e4-e517-420a-9496-37335e535ba6" />


这里的另一个RNN实际上在扮演的角色是一个语言模型LM。它只看token输出。我们可以用大量的文本语料去训练这个RNN，再和编码器一起去做训练。这样一来，我们就能确保接上语言模型的RNN-T的输出不会有空类别，这样对训练数据的标注就会容易许多。

<img width="655" height="503" alt="image" src="https://github.com/user-attachments/assets/54b05139-7731-4a7d-aa33-f5894a689bfd" />


刚刚的CTC，RNA和RNN-T都是每次只读一个声学特征，这样效率很低。Neural Transducer做了一个改变，用注意力机制来允许模型一次能读很多个声学特征。

<img width="673" height="491" alt="image" src="https://github.com/user-attachments/assets/3994b424-e7a3-4375-8398-92570e7b20d6" />


首先我们会让声学特征通过编码器产生隐层输出，再对一个窗口的小范围隐层做注意力后，再输出给解码的RNN。若窗口大小内信息已经用完了，解码的RNN就会输出空类别。接下来我们再移动窗口，对下一个窗口内的隐层重复刚才的操作。

<img width="648" height="427" alt="image" src="https://github.com/user-attachments/assets/35601282-cb6a-4081-b950-2795d84fc52e" />


窗口大小要如何设计呢？Neural Transducer做了实验发现，没有注意力时，窗口一长，结果就会烂掉。但用了注意力后，窗口大小又没那么重要。它尝试了各种计算注意力的方法，有DOT和MLP，甚至还有LSTM-ATTENTION。它把注意力权重读进LSTM中去，通过LSTM来考虑前一个时间步的注意力的位置。

<img width="645" height="448" alt="image" src="https://github.com/user-attachments/assets/257327db-76b2-4c6b-8b57-d578eae0ac20" />


MoChA是Monotoninc Chunkwise Attention的缩写。它能动态地调整移动窗口的距离。它用的是一个类似注意力机制的模块来做这个调整的操作。这个模块输入z0和隐层h1，输出yes/no，表示要不要把窗口开头放在此处位置。如果不要，就往右移动窗口，再检查下一个位置的隐层h2。一旦确定放置窗口，就对窗口内的隐层向量做注意力，解码出的token不会包含空类别。
