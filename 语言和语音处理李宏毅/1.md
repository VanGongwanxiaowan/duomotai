语音->文字

语音-》哪一个类别

文字——》

<img width="882" height="636" alt="image" src="https://github.com/user-attachments/assets/ce33b0c5-4bbf-4cec-876c-ee14e79c557f" />

speech recognition
# asr（automatic speech recognition)


<img width="890" height="300" alt="image" src="https://github.com/user-attachments/assets/3cfced51-6aab-4e7e-a6a4-0b6d544d3113" />

语音变式

text to speech sythentic

深度学习的技术做语音合成

输入一段声音信号，输出一段声音信号的
## unsupervised voice conversion

speajer recognition

keyword spotting

text-generation

auturegressive

non-autoregressive

<img width="870" height="609" alt="image" src="https://github.com/user-attachments/assets/e51b3264-7e76-40f1-a6e0-5fe39e225b38" />


meta learning

<img width="870" height="593" alt="image" src="https://github.com/user-attachments/assets/a35c64e1-d037-4d97-b52a-469342bf7228" />

<img width="846" height="461" alt="image" src="https://github.com/user-attachments/assets/a6236434-f257-4869-a31c-c072042eb8ec" />

adbersarial attack




文本和语音统称为人类语言。大部分NLP书籍Text和Speech侧重比为9:1，而李宏毅的这门课的侧重比为1:1。所以这门课叫人类语言处理。语音也是人类语言的很重要的部分。世界上大约有44%的语言只有语音没有文字书写系统。而且人类的语音很复杂。一段语音一秒钟有16K的采样点，每个采样点有256个可能的取值。没有人可以把同一段两次都说成完全相同。哪怕简单地说“你好”，每次的声波形状都会不同。人类文字也可以非常的复杂。世界上存在很长很长的句子，有的甚至可以长到13955个单词。比较最长的句子是没有意义的，你永远可以创造更长的句子，只需要不断地引用复述就好了。

<img width="644" height="373" alt="image" src="https://github.com/user-attachments/assets/0d589d35-49f1-4545-8939-5b461b51b205" />


虽然人类的语言可以如此复杂，但它实际的任务可以被分类得非常简单。语音和文本都可以在语音，文本和类别中通过模型互相转换。中间的转码器用深度神经网络硬train一发就好了。前提是给足标注的数据和GPU计算资源。文本到文本的任务应用有很多种，比如翻译，摘要，聊天机器人和问答系统等。文本到类别的任务包括主题分类，情感分类，质量审核分类，细粒度分类和意图识别等等。这里主要说一下了解不够多的语音到语音，文本和类别的转换。此外，这门课还会覆盖到元学习，风格迁移，知识图谱，对抗攻击和可解释学习等内容。

从语音到文本的任务叫语音辨识ASR。传统的语音辨识系统包含了很多不同的模组。我们必须把每个模组都学通，再用一条pipline组合起来，才能做语音辨识。这些模组包括了声学模型，语言模型和词典，以及还要知道怎么抽取语言的特征。而深度学习模型可以直接取代掉这些步骤。我们手机上的语音便是系统就是一个端对端的神经网络模型。过去传统的语音辨识系统各个模组加起来要消耗2GB的内存，而仅使用一个神经网络只需要不到80MB。所以它可以被嵌入到手机中使用。

<img width="666" height="464" alt="image" src="https://github.com/user-attachments/assets/41ff7290-c7cf-45d1-be15-138c6e4f6546" />


我们可以输入文字，输出语音做语音合成TSS。但直接用深度学习模型做语音合成会有一些想不到的问题。对于长句，机器能自动学到读这个长句的抑扬顿挫的语气和语调。但对于只有一个词的短句，机器表现通常会很不好。要不破音，要不少了尾部发音。深度学习模型多数情况合成的语音是好的，但有时会出现很奇怪的例外。一个原因是模型的表现很依赖于数据。训练数据中的短句或单词不足，导致模型对短句的语音合成表现奇怪。

我们也可以输入语音再输出语音。典型的任务包括语音分隔SS，和音色迁移VC。SS做的是把两个同时说话的声音分隔开来。在现实中，人会听到好几个不同的人说话，却可以专注于其中某一个人说的话。这个连傅里叶变换都不需要，直接深度学习端对端训练下来效果就很不错。VC任务类似于柯南的领结变声器。模型能保证说话人说的内容不变，只是嗓音发生了变化，比如声男变女声。甚至我们还可以做同声翻译。说话人是男声说中文，模型可以直接把它转换为说英文的女声。










