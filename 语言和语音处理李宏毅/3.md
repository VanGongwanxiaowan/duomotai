# LAS

seq2seq

# CTC
decoder是linear classifier的seq2seq

# rna

输入一个东西就是要输出一个东西的seq2seq

# rna-t

输入一个东西可以输出多个东西的seq2seq

# neural transducer

每次输入一个window的rnn-t


<img width="649" height="364" alt="image" src="https://github.com/user-attachments/assets/1f2b3bb4-f8b9-4521-9166-0106dae3b168" />


常用的语音识别模型有两种，一种是基于Seq2Seq的模型，一种是基于HMM。Seq2Seq的模型有LAS，CTC，RNN-T等。19年最流行的模型是LAS，其次是CTC。

LAS是Listen, Attention, and Spell 的缩写，模型架构是一个中间加了注意力层的自编码器。编码器会把输入的一串声学特征，转换为高维隐层嵌入。它的主要目标是提取出内容信息，过滤掉说话者的嗓音变化和环境噪音。编码器我们可以用CNN和RNN，也可以是CNN+RNN，也可以用自注意力层。我们通常会对输入做下采样。

<img width="651" height="469" alt="image" src="https://github.com/user-attachments/assets/105ec5f1-2303-4b7a-a79c-d47463607f7c" />


因为一段声音讯号太长了，而且相邻向量间，带有重复的信息。为了节省计算量 ，我们可以用Pyramid RNN。它的做法是在每一层的RNN输出后，都做一个聚合操作。把两个向量加起来，变成一个向量。这样它的隐层就会比较少。或者我们也可以用Pooling Over time，两个time step的向量，只选其中一个，输入到下一层。

<img width="657" height="463" alt="image" src="https://github.com/user-attachments/assets/140cd8b3-22b6-4a64-a5e6-c8bef6a483b6" />


Time-delay DNN是CNN常用的一个变形。通常CNN是计算一个窗口内每个元素的加权之和，而TDDNN则只计算第一个和最后一个元素。Truncated self-attention是自注意力的一个变形。通常自注意力会对一个序列中每个元素都去注意，而Truncated的做法是只让当前元素去对周边一个窗口范围的元素注意。

<img width="642" height="374" alt="image" src="https://github.com/user-attachments/assets/24485411-134b-418e-bc3c-11918129ed02" />


编码器和解码器中间的注意层会有一个Z0参数作为要搜索的Query，而编码器输出的隐层嵌入每位置的向量hi是要注意的Key。我们用Z0和每一个hi去计算注意力得分，注意力得分计算方式可以是两个向量通过线性变换后的点积运算。如果是加性注意力，也可以是线性变换后相加，再通过一个tanh，再做一次线性变换获得注意力分数。我们把每个位置计算得到的注意力分数向量通过softmax，就可以得到一个归一化的分布C0。

<img width="682" height="495" alt="image" src="https://github.com/user-attachments/assets/89f08ca1-7b1f-4939-a6d7-37b821d3b551" />


这个归一化的分布向量和之前的Z0会作为解码器RNN的输入，输出是隐层Z1，和一个对词表V中所有可能词预测的概率分布向量。我们取max就可以解码得到最可能的第一个token。再拿Z1与原编码器的隐层向量做注意力，得到一个新的注意力分布C1。它与C1一同输入给RNN，同样的方式就能解码得到第二个token。以此类推，直到解码得到的token是一个终止符<EOS>，就结束。

<img width="685" height="293" alt="image" src="https://github.com/user-attachments/assets/029c1bee-2570-421d-84cf-dc3666b79f6a" />


若我们每次都在当前位置直接取max，基于贪心的解码策略不能保证生成整个序列的概率是最优的。但实际中，我们的词表很大，没有办法去搜索穷尽所有的可能性。解决这个问题的方法就是使用Beam Search？它是窗口大小为K的贪心搜索。从每个节点我们都保留K个最好的路径，一直往下。

<img width="676" height="499" alt="image" src="https://github.com/user-attachments/assets/f63b2b51-1fc5-42b3-b516-e7ad087e2548" />


训练时，我们的标签是序列长度为N的one-hot向量。它会与解码器RNN第一步输出的大小为词表V的概率分布计算交叉熵损失。在进行下一步RNN解码时，上一步的正确标签c会替代最大概率的C0进行解码。这个技术叫作Teacher Forcing。

<img width="671" height="444" alt="image" src="https://github.com/user-attachments/assets/fafa27ad-d84c-4a9e-b34c-b9a8fc296b96" />


为什么要这样去训练呢？因为如果我们拿前一个位置的概率分布max输出，当作下一个时间步RNN的输入。模型一开始参数是从随机开始训练，解码器表现很烂，输出的东西都是很乱的，这会影响下一步的解码。比如一开始RNN每次都输出x，下一步解码根据x再输出a。但经过一连串的训练以后，解码器渐渐变厉害了。它可以输出正确答案c，并告诉后面的RNN说，"看到输入c，你应该输出a"，但这与之前学到的"看到输入x，你应该输出a"就冲突了。所以最好的方式就是不要理睬RNN的输出，用正确标签去指导它去做下一步的预测。

<img width="672" height="496" alt="image" src="https://github.com/user-attachments/assets/1e8ade15-704d-4ce7-8ab4-a3221426bed4" />


之前的注意力阶段，我们每次是用解码器的输出隐层去与编码器的输出做注意力。除此以外，还有另一种做注意力的方式。我们把解码器的隐层Zt拿出来与hi做注意力得到Ct。这个Ct不是保留到下一个时间才使用，在当前时间点立刻使用。我们把Zt和Ct丢给解码器RNN，得到新分布Zt+1。这两种注意力的区别在，注意力得到的结果是下一个时间使用还是当前时间使用。第一篇拿Seq2Seq做语音识别的论文，用的是二者的合体版本。

<img width="656" height="365" alt="image" src="https://github.com/user-attachments/assets/b69dd186-6167-4c55-a4eb-6f07e6ec71c2" />


语音识别是否非要用注意力不可呢？注意力最早是用在Seq2Seq翻译上解决源语言与目标语言的对齐问题。这个弹性很大的注意力，在语音上用会有种杀鸡焉用牛刀的赶脚。因为语音上，每次注意跳跃是有限的。而不会出现像机器翻译那样，开头注意到结尾的的大跳跃情况。我们可以用Location-aware attention来优化。我们的注意力不能够随便乱跳，而是要考虑前一个时间步得到的注意力权重影响。我们把t之前的注意力权重α0到αt-1的向量，做一个线性映射后再输入给解码RNN。 这样模型就能学到，每解码出一个token，注意力就要往右移动一点。

<img width="686" height="283" alt="image" src="https://github.com/user-attachments/assets/8fec9cbb-ed0f-4d2f-af88-cb126409678c" />


LAS模型需要在海量数据集上训练，和一些调参技巧，与传统方法相比才会有显著提升。但LAS有另一个好处是，它的模型参数可以比传统方法变得很小。

<img width="664" height="494" alt="image" src="https://github.com/user-attachments/assets/9418b1b2-e905-4d1c-8251-1506e1b4adae" />


LAS的注意力可视化出来发现，即便没有用Location-aware attention，模型也可以自己学到这样的注意规律。还有一个有趣的发现，模型能够自动识别出aaa和triple a是对应一样的声音讯号。这可以解释为，二者的上下文经常一样。 LAS可以学到复杂的输入讯号和输出讯号的关系。

<img width="687" height="409" alt="image" src="https://github.com/user-attachments/assets/c4d38162-ce45-4eb4-8fb4-8060b7ab9a28" />


LAS模型也可以用语言模型 LM来提升。不过LAS本身很强，在给足训练集的情况下，不用LM来优化也是能表现很好的。我们还可以用LAS模型来训练把闽南话语音翻译成中文字幕，或者是很多电影里面自动把英语语音翻译成中文字幕。即便背景音存在音乐和音效，语音和字幕有时没有对齐，我们都可以无需去管。直接海量数据，深度学习模型硬train一发，以上问题统统都能自动解决。

<img width="684" height="454" alt="image" src="https://github.com/user-attachments/assets/e9b10f0a-2734-483d-be15-7d56b2e7ffbe" />


LAS虽然神通广大，但它也有一些问题。我们期待我们的模型可以做online的识别，即能够一边听，一边做语音识别。而不是模型听完整句话后，等上一秒，模型才输出辨识结果。往后要讲的模型，就是解决LAS的这个问题。
