```
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import CIFAR10
from torchvision.transforms import Compose, ToTensor, Normalize, Resize
import numpy as np

# 设备配置
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 超参数设置
image_size = 32  # CIFAR10图像尺寸
patch_size = 4   # 每个patch的大小
num_patches = (image_size // patch_size) ** 2  # 32/4=8，共8x8=64个patch
in_channels = 3  # RGB图像
embed_dim = 128  # 嵌入维度
num_heads = 4    # 注意力头数
num_layers = 4   # Transformer层数
mask_ratio = 0.75  # 掩码比例（75%的patch被掩码）
lr = 1e-4
batch_size = 64
epochs = 50

# 1. 图像分块与嵌入
class PatchEmbed(nn.Module):
    def __init__(self, image_size, patch_size, in_channels, embed_dim):
        super().__init__()
        self.image_size = image_size
        self.patch_size = patch_size
        self.num_patches = (image_size // patch_size) ** 2
        
        # 使用卷积将每个patch转换为嵌入向量
        self.proj = nn.Conv2d(
            in_channels, 
            embed_dim, 
            kernel_size=patch_size, 
            stride=patch_size
        )

    def forward(self, x):
        # x shape: (batch_size, 3, 32, 32)
        x = self.proj(x)  # 输出: (batch_size, embed_dim, 8, 8)
        x = x.flatten(2)  # 输出: (batch_size, embed_dim, 64)
        x = x.transpose(1, 2)  # 输出: (batch_size, 64, embed_dim)
        return x

# 2. MAE编码器（仅处理可见patch）
class Encoder(nn.Module):
    def __init__(self, embed_dim, num_heads, num_layers):
        super().__init__()
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=num_heads,
                dim_feedforward=4*embed_dim,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=num_layers
        )

    def forward(self, x):
        # x shape: (batch_size, num_visible_patches, embed_dim)
        return self.transformer(x)

# 3. MAE解码器（重建完整图像）
class Decoder(nn.Module):
    def __init__(self, embed_dim, decoder_embed_dim, num_heads, num_layers, patch_size, in_channels):
        super().__init__()
        self.proj = nn.Linear(embed_dim, decoder_
        self.proj = nn.Linear(embed_dim, decoder_embed_dim)  # 投影到解码器维度
        self.mask_token = nn.Parameter(torch.randn(1, 1, decoder_embed_dim))  # 掩码token
        
        # 位置嵌入（解码器专用）
        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, decoder_embed_dim))
        
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=decoder_embed_dim,
                nhead=num_heads,
                dim_feedforward=4*decoder_embed_dim,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=num_layers
        )
        
        # 最终投影到patch像素值
        self.head = nn.Linear(decoder_embed_dim, patch_size**2 * in_channels)

    def forward(self, x, ids_restore):
        # x: 编码器输出 (batch_size, num_visible, embed_dim)
        # ids_restore: 用于恢复原始patch顺序的索引
        
        # 投影到解码器维度
        x = self.proj(x)  # (batch_size, num_visible, decoder_embed_dim)
        
        # 生成掩码token并插入
        batch_size, num_visible, _ = x.shape
        num_masked = num_patches - num_visible
        
        # 扩展掩码token到批次大小
        mask_tokens = self.mask_token.repeat(batch_size, num_masked, 1)  # (batch_size, num_masked, decoder_embed_dim)
        
        # 拼接可见patch和掩码token
        x_ = torch.cat([x, mask_tokens], dim=1)  # (batch_size, num_patches, decoder_embed_dim)
        
        # 恢复原始patch顺序
        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_.shape[2]))  # (batch_size, num_patches, decoder_embed_dim)
        
        # 添加位置嵌入
        x_ = x_ + self.pos_embed
        
        # 解码器Transformer
        x_ = self.transformer(x_)  # (batch_size, num_patches, decoder_embed_dim)
        
        # 重建像素值
        pred = self.head(x_)  # (batch_size, num_patches, patch_size^2 * 3)
        return pred

# 4. MAE主模型
class MAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.patch_embed = PatchEmbed(image_size, patch_size, in_channels, embed_dim)
        
        # 编码器位置嵌入
        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim))
        
        self.encoder = Encoder(embed_dim, num_heads, num_layers)
        
        # 解码器参数（通常解码器可以更轻量）
        decoder_embed_dim = embed_dim // 2
        self.decoder = Decoder(embed_dim, decoder_embed_dim, num_heads, num_layers, patch_size, in_channels)

    def random_masking(self, x):
        """随机掩码patch"""
        batch_size, num_patches, embed_dim = x.shape
        
        # 生成随机掩码
        num_masked = int(mask_ratio * num_patches)
        # 为每个样本生成不同的掩码索引
        noise = torch.rand(batch_size, num_patches, device=x.device)  # (batch_size, num_patches)
        
        # 按噪声值排序，获取掩码和可见索引
        ids_shuffle = torch.argsort(noise, dim=1)  # 升序排序，小值先被掩码
        ids_restore = torch.argsort(ids_shuffle, dim=1)  # 用于恢复原始顺序
        
        # 区分掩码和可见patch
        ids_keep = ids_shuffle[:, :num_patches - num_masked]  # 可见patch索引
        x_visible = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, embed_dim))  # 可见patch
        
        # 生成掩码标记（用于损失计算）
        mask = torch.ones([batch_size, num_patches], device=x.device)
        mask[:, :num_patches - num_masked] = 0
        mask = torch.gather(mask, dim=1, index=ids_restore)  # 恢复掩码的原始位置
        
        return x_visible, mask, ids_restore

    def forward(self, x):
        # 1. 图像分块与嵌入
        x = self.patch_embed(x)  # (batch_size, num_patches, embed_dim)
        
        # 2. 添加位置嵌入
        x = x + self.pos_embed
        
        # 3. 随机掩码
        x_visible, mask, ids_restore = self.random_masking(x)
        
        # 4. 编码器处理可见patch
        z = self.encoder(x_visible)  # (batch_size, num_visible, embed_dim)
        
        # 5. 解码器重建
        pred = self.decoder(z, ids_restore)  # (batch_size, num_patches, patch_size^2 * 3)
        
        return pred, mask

# 5. 损失函数（仅计算掩码区域的MSE）
def mae_loss(pred, target, mask):
    # pred: 重建的patch (batch_size, num_patches, patch_size^2 * 3)
    # target: 原始patch (batch_size, num_patches, patch_size^2 * 3)
    # mask: 掩码标记 (batch_size, num_patches)，1表示被掩码的位置
    
    loss = (pred - target) ** 2
    loss = loss.mean(dim=-1)  # 每个patch的MSE
    loss = (loss * mask).sum() / mask.sum()  # 只计算掩码区域的损失
    return loss

# 6. 数据准备
def prepare_data():
    transform = Compose([
        Resize(image_size),
        ToTensor(),
        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    train_dataset = CIFAR10(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4
    )
    
    return train_loader

# 7. 训练函数
def train():
    train_loader = prepare_data()
    model = MAE().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)
    
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for batch_idx, (images, _) in enumerate(train_loader):
            images = images.to(device)
            batch_size = images.shape[0]
            
            # 生成目标patch（用于计算损失）
            # 将原始图像转换为patch形式
            patches = model.patch_embed(images)  # (batch_size, num_patches, embed_dim) - 这里只是用分块功能
            # 重新将patch转换为像素值形式
            target = patches.reshape(batch_size, num_patches, -1)  # 简化处理，实际应对应原始像素
            
            # 前向传播
            pred, mask = model(images)
            
            # 计算损失
            loss = mae_loss(pred, target, mask)
            
            # 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # 打印进度
            if batch_idx % 100 == 0:
                print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')
        
        avg_loss = total_loss / len(train_loader)
        print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_loss:.4f}')
    
    # 保存模型
    torch.save(model.state_dict(), 'mae_cifar10.pth')
    print("模型保存完成")

if __name__ == "__main__":
    train()

```

某些patch mask掉了

定义mask的时候，可以学习的parameter的，
