<img width="928" height="453" alt="image" src="https://github.com/user-attachments/assets/f4a9b596-1527-421c-aeed-d2c36b4c9e53" />

<img width="927" height="422" alt="image" src="https://github.com/user-attachments/assets/e064dadc-5b2a-4284-b4e6-d48db2e3a4ac" />

```
import torch
import torch.nn as nn
import timm
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
import torchvision.transforms as transforms
class AdaptMLP(nn.Module):
    def __init__(self,hidden_dim):
        super(AdaptMLP,self).__init__()
        self.down_proj = nn.Linear(input_dim,hidden_dim)
        self.relu = nn.ReLU(inplace=True)
        self.up_proj = nn.Linear(hidden_dim,input_dim)
    def forward(self,x):
        x=self.down_proj(x)
        x=self.relu(x)
        x=self.up_proj(x)
        return x
class Adapter(nn.Module):
    def __init__(self,num_claasses,hidden_dim):
        super(Adapter,self).__init__()
        self.vit = trim.create_model('vit_small_patch16_224',preTrained=True)
        for param in self.vit.parameters():
            param.requires_grad = False
        self.adapter_mlps = []
        for i ,block in enumerate(self.vit.blocks):
            params.requires_grad = True
        self.adapter_mlps.append(AdapterMLP(input_dim = self.vits.blocks[i],hidden_dim = hidden_dim))
        self.vit.head = nn.Linear(self.vit.head.in_features,num_classes)
    def forward(self,x):
        x=self.vit.patch_embed(x)
        if self.vit.cls_token is not None:
            cls_token = self.vit.cls_token.expand(x.shape[0],-1,-1)
            x = torch.cat([cls_token,x],dim=1)
        if self.vit.pos_embed is not None:
            x = x+self.vit.pos_emved.expand(x.shape[0],-1,+1)
        x = self.vit.pos_drop(x)
        for i ,block in enumerate(self.vit.blocks):
            block_input = x
            x = block.norml(x)
            x = block.attn(x)
            x = block_input + x
            original_mlp_output = block.norm2(x)
            original_mlp_output = block.mlp(orginal_mlp_output)
            adapter_output = self.adapter_mlps[i](x)
            x = original_mlp_output + adapter_output
        x = self.vit.norm(x)
        if self.vit.clas_token is not None:
            x = x[:,0]
        x = self.vit.head(x)
        return x


def load_cifar10_dataset():
    transform = transforms.Compose([transforms.Resize([224,224]),transforms.ToTensor()])
    train_dataset = CIFAR10(root='./data',train=True,download=True,transform=transform)
    test_dataset = CIFAR10(root='./data',train=False,download=True,transform=transform)
    loader = DataLoader(train_dataset,batch_size=4,shuffle=True)
    return loader
def main():
    dataset = load_cifar10_dataset()
    model = FineTuning(num_classes=10)
    for images,labels in dataset:
        logits = model(images)
        loss = torch.nn.CrossEntropyLoss()(logits,labels)
        optimizer.zero_grad()
        loss.backward()
    model = Adapter(num_classes=10,hidden_size = 384)
    for images.labels in dataset:
        logits = model(images)
        loss = nn.CrossEntropyLoss()(logits,labels)
        print(loss)
```
