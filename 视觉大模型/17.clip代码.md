<img width="913" height="429" alt="image" src="https://github.com/user-attachments/assets/6088440d-65ce-4352-b5a2-ed900cec3910" />

文本embedding和图像embedding是不一样的，所以之后做一个全连阶层，变成相同的维度。
```
class ViT(nn.Module):
    def _init_(self,output_dim):
        super().__init__()
        self.output_dim = output_dim
        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True,num_classes=output_dim)
        self.vit.head = nn.Linear(self.vit.head.in_features, output_dim)

    def forward(self, x):
        return self.vit(x)
class TextEncoder(nn.Module):
    def _init_(self):
        super(TextEncoder,self).__init__()
        BERT_LOCH_PATH ='./ber-base-uncased'
        self.model = BertModel.from_pretrained(BERT_LOCAL_PATH)
        self.tokenizer=BertTokenier.from_pretrained(BERT_LOCAL_PATH)
    def forward(self,texts):
        encoded_inputs=self.tokenizer(texts,return_tensor='pt',padding=True,truncation=True)
        outputs=self.model(**encoded_inputs)
        return outputs.last_hidden_state[:,0,:]

def load_cifar10_dataset():
    transform = transform.Compose([transforms.Resize((224,224)),transform.ToTensor()])
    train_dataset = datasets.CIFAR10(root='./data',train=True,download=True,transform=transform)
    loader = DataLoader(train_dataset,batch_size=4,shuffle=True)
    classes=train_dataset.classes
    return loader,classes
class CLIP(nn.Module):
    def _init_(self,image_output_dim,text_output_dim):
        super(CLIP,self)._init_()
        self.image_encoder = ViT(image_output_dim)
        self.text_decoder=TextEncoder()
        self.W_i = nn.Parameter(torch.randn(Image_output_dim,text_output_dim))
        self.W_t = nn.Parameter(torch.randn(768,text_output_dim))
    def forward(self,images,texts):
        I_f = self.image_encoder(images)
        T_f = self.text_encoder(texts)
        I_e = torch.matmul(I_f,self.W_i)#(B,512)
        I_e = torch.matmul(T_f,self.W_t)#(B,512_
        logits = torch.matmul(I_e,T_e.T)
        return logits


def main():
    dataset,classes = load_cifar10_dataset()
    clip_model = CLIP(image_output_dim=512,text_output_dim=512)
    for images, labels in dataset:
        texts = [classes[label] for label in labels]
        logits = clip_model(images,texts)

```
